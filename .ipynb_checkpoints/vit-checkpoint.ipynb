{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import datasets,transforms\n",
    "import time\n",
    "from torch.nn import functional as F\n",
    "from math import floor,ceil\n",
    "import math\n",
    "from collections import OrderedDict\n",
    "from einops import rearrange,reduce,repeat\n",
    "from einops.layers.torch import Rearrange,Reduce\n",
    "from torchvision.transforms import Compose,Resize,ToTensor\n",
    "\n",
    "device=torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "num_epochs=60\n",
    "batch_size=100\n",
    "learnin_rate=0.001\n",
    "global X_liner #定义特征收集器\n",
    "\n",
    "#数据增强\n",
    "class RandomErasing(object):\n",
    "    '''\n",
    "    Class that performs Random Erasing in Random Erasing Data Augmentation by Zhong et al.\n",
    "    -------------------------------------------------------------------------------------\n",
    "    probability: The probability that the operation will be performed.\n",
    "    sl: min erasing area\n",
    "    sh: max erasing area\n",
    "    r1: min aspect ratio\n",
    "    mean: erasing value\n",
    "    -------------------------------------------------------------------------------------\n",
    "    '''\n",
    "    def __init__(self,probability=0.5,sl=0.02,sh=0.4,rl=0.3,mean=[0.4914,0.4822,0.4465]):\n",
    "        self.probability=probability\n",
    "        self.sl=sl\n",
    "        self.sh=sh\n",
    "        self.rl=rl\n",
    "        self.mean=mean\n",
    "        \n",
    "    def __call__(self,img):\n",
    "        \n",
    "        if random.uniform(0,1)>self.probability:#判断是否填充\n",
    "            return img\n",
    "        \n",
    "        for attempt in range(100):\n",
    "            area=img.size()[1]*img.size()[2]\n",
    "            target_area=random.uniform(self.sl,self.sh)\n",
    "            aspect_ratio=random.uniform(self.rl,1/self.rl)\n",
    "            \n",
    "            h=int(round(math.sqrt(target_area*aspect_ratio)))\n",
    "            w=int(round(math.sqrt(target_area/aspect_ratio)))\n",
    "            \n",
    "            if w<=img.size()[2] and h<=img.size()[1]: #填充缩放的图像\n",
    "                x1=random.randint(0,img.size()[1]-h)\n",
    "                y1=random.randint(0,img.size()[2]-w)\n",
    "                if img.size()[0]==3:\n",
    "                    img[0,x1:x1+h,y1:y1+w]=self.mean[0]\n",
    "                    img[1,x1:x1+h,y1:y1+w]=self.mean[1]\n",
    "                    img[2,x1:x1+h,y1:y1+w]=self.mean[2]\n",
    "                else:\n",
    "                    img[0,x1:x1+h,y1:y1+w]=self.mean[0]\n",
    "                return img\n",
    "            \n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#金字塔池化，解决缩放图像产生的问题\n",
    "class SpatialPyramidPooling2d(nn.Module):\n",
    "    def __init__(self,num_level,pool_type='max_pool'):\n",
    "        super(SpatialPyramidPooling2d,self).__init__()\n",
    "        self.num_level=num_level\n",
    "        self.pool_type=pool_type\n",
    "        \n",
    "    def forward(self,x):\n",
    "        N,C,H,W=x.size()\n",
    "        \n",
    "        #做第i层的金字塔池化\n",
    "        for i in range(self.num_level): \n",
    "            level=i+1\n",
    "            kernel_size=(ceil(H/level),ceil(W/level))\n",
    "            stride=(ceil(H/level),ceil(W/level))\n",
    "            padding=(floor(kernel_size[0]*level-H+1)/2,floor(kernel_size[1]*level-W+1))\n",
    "            \n",
    "            if self.pool_type=='max_pool':\n",
    "                tensor=(F.max_pool2d(x,kernel_size=kernel_size,stride=stride,padding=padding)).view(N,-1)\n",
    "            else:\n",
    "                tensor=(F.avg_pool2d(x,kernel_size=kernel_size,stride=stride,padding=padding)).view(N,-1)\n",
    "            \n",
    "            if i==0: #堆叠金字塔\n",
    "                res=tensor\n",
    "            else:\n",
    "                res=torch.cat((res,tensor),1)\n",
    "                \n",
    "        return res\n",
    "    \n",
    "spp_layer=SpatialPyramidPooling2d(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载数据\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.FashionMNIST('./Fashion_MNIST', train=True, download=True,\n",
    "                               transform=torchvision.transforms.Compose([\n",
    "                                   torchvision.transforms.RandomCrop(28, padding=2),\n",
    "                                   torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                   torchvision.transforms.ToTensor(),\n",
    "                                   torchvision.transforms.Normalize((0.1307,), (0.3081,)),\n",
    "                                   RandomErasing(probability = 0.1, mean = [0.4914]),\n",
    "                               ])),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.FashionMNIST('./Fashion_MNIST/', train=False, download=True,\n",
    "                               transform=torchvision.transforms.Compose([\n",
    "                                   torchvision.transforms.ToTensor(),\n",
    "                                   torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "                               ])),\n",
    "    batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "在检查了最初的实现之后，我发现为了提高性能，作者使用了Conv2d层而不是线性层,\n",
    "这是通过使用与“patch_size”相等的kernel_size和stride 来获得的。\n",
    "直观地说，卷就是积运算分别应用于每个patch。\n",
    "因此，这里首先应用conv层，然后将生成的图像展平\n",
    "'''\n",
    "#创建线性映射层进行投影\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels: int = 3, patch_size: int = 16, emb_size: int = 768, img_size: int = 224):\n",
    "        self.patch_size = patch_size\n",
    "        super().__init__()\n",
    "        self.projection = nn.Sequential(\n",
    "            # 用卷积层代替线性层以提升性能\n",
    "            nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=patch_size),\n",
    "            # 先用大小为切片大小，步长为切片步长的卷积核来提取特征图，然后将特征图展平\n",
    "            Rearrange('b e (h) (w) -> b (h w) e'),\n",
    "        )\n",
    "        self.cls_token = nn.Parameter(torch.randn(1,1, emb_size))\n",
    "        self.positions = nn.Parameter(torch.randn((img_size // patch_size) **2 + 1, emb_size))\n",
    "        \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        b, _, _, _ = x.shape\n",
    "        x = self.projection(x)\n",
    "        cls_tokens = repeat(self.cls_token, '() n e -> b n e', b=b)\n",
    "        x = torch.cat([cls_tokens, x], dim=1) #在输入前添加cls标记\n",
    "        x += self.positions #加位置嵌入\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#实现多头注意力\n",
    "class MutiHeadAttention(nn.Module):\n",
    "    def __init__(self,emb_size:int=768,num_heads:int=8,dropout:float=0):\n",
    "        super().__init__()\n",
    "        self.emb_size=emb_size\n",
    "        self.num_heads=num_heads\n",
    "        self.qkv=nn.Linear(emb_size,emb_size*3) #将query、key和value融合到一个矩阵中\n",
    "        self.att_drop=nn.Dropout(dropout)\n",
    "        self.projection=nn.Linear(emb_size,emb_size)\n",
    "        \n",
    "    def forward(self,x:Tensor,mask:Tensor=None)->Tensor:\n",
    "        #分割num_heads中的query、key和value\n",
    "        qkv=rearrange(self.qkv(x),'b n (h d qkv) -> (qkv) b h n d',h=self.num_heads,qkv=3)\n",
    "        queries,keys,values=qkv[0],qkv[1],qkv[2]\n",
    "        \n",
    "        #在最后一个轴上求和\n",
    "        energy=torch.einsum('bhqd, bhkd -> bhqk',queries,keys)\n",
    "        if mask is not None:\n",
    "            fill_value=torch.finfo(torch.float32).min\n",
    "            energy.mask_fill(~mask,fill_value)\n",
    "            \n",
    "        #计算多头注意力    \n",
    "        scaling=self.emb_size**(1/2)\n",
    "        att=F.softmax(energy,dim=-1)/scaling\n",
    "        att=self.att_drop(att)\n",
    "        \n",
    "        #在第三个轴上求和\n",
    "        out=torch.einsum('bhal, bhlv -> bhav ',att,values)\n",
    "        out=rearrange(out,'b h n d -> b n (h d)')\n",
    "        out=self.projection(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#残差连接\n",
    "class ResidualAdd(nn.Module):\n",
    "    def __init__(self,fn):\n",
    "        super().__init__()\n",
    "        self.fn=fn\n",
    "        \n",
    "    def forward(self,x,**kwargs):\n",
    "        res=x\n",
    "        x=self.fn(x,**kwargs)\n",
    "        x+=res\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#反馈组件\n",
    "class FeedForwardBlock(nn.Sequential):\n",
    "    def __init__(self,emb_size:int,expansion:int=4,drop_p:float=0.):\n",
    "        super().__init__(\n",
    "            nn.Linear(emb_size,expansion*emb_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(drop_p),\n",
    "            nn.Linear(expansion*emb_size,emb_size)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#创建Transformer编码器块\n",
    "class TransformerEncoderBlock(nn.Sequential):\n",
    "    def __init__(\n",
    "        self,\n",
    "        emb_size:int=768,\n",
    "        drop_p:float=0.,\n",
    "        forward_expansion:int=4,\n",
    "        forward_drop_p:float=0.,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(\n",
    "            ResidualAdd(\n",
    "                nn.Sequential(\n",
    "                    nn.LayerNorm(emb_size),\n",
    "                    MutiHeadAttention(emb_size,**kwargs),\n",
    "                    nn.Dropout(drop_p)\n",
    "                )),\n",
    "            ResidualAdd(\n",
    "                nn.Sequential(\n",
    "                    nn.LayerNorm(emb_size),\n",
    "                    FeedForwardBlock(emb_size,expansion=forward_expansion,drop_p=forward_drop_p),\n",
    "                    nn.Dropout(drop_p)\n",
    "                ))\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#创建Transformer编码器\n",
    "class TransformerEncoder(nn.Sequential):\n",
    "    def __init__(self,depth:int=12,**kwargs):\n",
    "        super().__init__(*[TransformerEncoderBlock(**kwargs) for _ in range(depth)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#分类器\n",
    "class ClassificationHead(nn.Sequential):\n",
    "    def __init__(self,emb_size:int=768,n_classes:int=1000):\n",
    "        super().__init__(\n",
    "            Reduce('b n e -> b e',reduction='mean'),\n",
    "            nn.LayerNorm(emb_size),\n",
    "            nn.Linear(emb_size,n_classes)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#组成PatchEmbedding、TransformerEncoder和ClassificationHead来创建最终的ViT\n",
    "class ViT(nn.Sequential):\n",
    "    def __init__(\n",
    "        self,in_channels:int=1,patch_size:int=7,emb_size:int=768,\n",
    "        img_size:int=28,depth:int=6,n_classes:int=10,**kwargs\n",
    "    ):\n",
    "        super().__init__(\n",
    "            PatchEmbedding(in_channels,patch_size,emb_size,img_size),\n",
    "            TransformerEncoder(depth,emb_size=emb_size,**kwargs),\n",
    "            ClassificationHead(emb_size,n_classes)\n",
    "        )\n",
    "        \n",
    "model=ViT().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义模型和损失函数\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=learnin_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#设置一个通过优化器更新学习率的函数\n",
    "def update_lr(optimizer,lr):\n",
    "    for param_groups in optimizer.param_groups:\n",
    "        param_groups['lr']=lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义测试函数\n",
    "def test(model,test_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct=0\n",
    "        total=0\n",
    "        for images,labels in test_loader:\n",
    "            images=images.to(device)\n",
    "            labels=labels.to(device)\n",
    "            \n",
    "            global X_liner #清空特征收集器\n",
    "            X_liner=torch.empty((batch_size,0),device=device)\n",
    "            \n",
    "            outputs=model(images)\n",
    "            _,predicted=torch.max(outputs.data,1)\n",
    "            total+=labels.size(0)\n",
    "            correct+=(predicted==labels).sum().item()\n",
    "            \n",
    "        print('测试图片的准确率: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-90efcd3f73e7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m def grad(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#训练模型更新学习率\n",
    "total_step=len(train_loader)\n",
    "curr_lr=learnin_rate\n",
    "for epoch in range(num_epochs):\n",
    "    in_epoch=time.time()\n",
    "    for i,(images,labels) in enumerate(train_loader):\n",
    "        images=images.to(device)\n",
    "        labels=labels.to(device)\n",
    "        \n",
    "        X_liner=torch.empty((batch_size,0),device=device)\n",
    "        outputs=model(images)\n",
    "        loss=criterion(outputs,labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1)%100==0:\n",
    "            print(\"Epoch [{}/{}], Step [{}/{}] Loss: {:.4f}\"\n",
    "                  .format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))\n",
    "        \n",
    "    test(model,train_loader)\n",
    "    out_epoch=time.time()\n",
    "    print(f\"use {(out_epoch-in_epoch)//60}min{(out_epoch-in_epoch)%60}s\")\n",
    "    \n",
    "    if (epoch+1)%20==0: #更新学习率\n",
    "        curr_lr/=3\n",
    "        update_lr(optimizer,curr_lr)\n",
    "        \n",
    "test(model,train_loader)\n",
    "torch.save(model.state_dict(), 'resnet.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
